name: CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test-kafka-event-producer-integration:
    runs-on: ubuntu-latest

    services:
      zookeeper:
        image: confluentinc/cp-zookeeper:7.5.0
        ports:
          - 2181:2181
        env:
          ZOOKEEPER_CLIENT_PORT: 2181
          ZOOKEEPER_TICK_TIME: 2000

      kafka:
        image: confluentinc/cp-kafka:7.5.0
        ports:
          - 9092:9092
        env:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
          KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Wait for Kafka to be ready
      run: |
        curl -s https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh -o wait-for-it.sh
        chmod +x wait-for-it.sh
        ./wait-for-it.sh localhost:9092 -t 30

    - name: Run Kafka producer/consumer test
      run: |
        cd src
        nohup python event_producer.py &
        pytest -s -v ../tests/integration/test_kafka_producer.py
  test-streaming-processor-integration:
    runs-on: ubuntu-latest

    services:
      zookeeper:
        image: confluentinc/cp-zookeeper:7.5.0
        ports:
          - 2181:2181
        env:
          ZOOKEEPER_CLIENT_PORT: 2181
          ZOOKEEPER_TICK_TIME: 2000

      kafka:
        image: confluentinc/cp-kafka:7.5.0
        ports:
          - 9092:9092
        env:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
          KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Set up Spark
      run: |
        wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
        tar -xzf spark-3.5.1-bin-hadoop3.tgz
        echo "SPARK_HOME=$PWD/spark-3.5.1-bin-hadoop3" >> $GITHUB_ENV
        echo "$SPARK_HOME/bin" >> $GITHUB_PATH


    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Start LocalStack
      run: |
        cd s3
        docker compose up -d

    - name: Wait for Kafka to be ready
      run: |
        curl -s https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh -o wait-for-it.sh
        chmod +x wait-for-it.sh
        ./wait-for-it.sh localhost:9092 -t 30

    - name: Create .env file
      run: |
        cd s3
        echo "AWS_ACCESS_KEY_ID=test" >> .env
        echo "AWS_SECRET_ACCESS_KEY=test" >> .env
        echo "AWS_DEFAULT_REGION=eu-central-1" >> .env
        echo "BUCKET_NAME=my-parametric-bucket" >> .env
        echo "ENDPOINT_URL=http://localhost:4566" >> .env
        echo "PROFILE=default" >> .env

    - name: Run events and Spark streaming processor
      run: |
        cd src
        set -a
        source ../s3/.env
        set +a
        spark-submit \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.2 \
          --conf spark.sql.streaming.metricsEnabled=true \
          --conf spark.metrics.conf=metrics.properties \
          --conf spark.driver.extraJavaOptions="-javaagent:jmx_prometheus_javaagent-1.4.0.jar=7071:config.yaml" \
          --conf spark.executorEnv.AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
          --conf spark.executorEnv.AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
          --conf spark.executorEnv.AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \
          --conf spark.executorEnv.BUCKET_NAME=$BUCKET_NAME \
          --conf spark.executorEnv.ENDPOINT_URL=$ENDPOINT_URL \
          --conf spark.executorEnv.PROFILE=$PROFILE \
          streaming_processor.py
        python ../tests/integration/spark_event_producer.py
  test-unit-and-coverage:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Run Unit test and test coverage
      run: |
        pytest --cov=src tests/unit/
