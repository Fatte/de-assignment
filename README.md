# DE Takehome Test: Event Streaming and Processing Pipeline

## ğŸ“– Project Overview

This project implements a complete event streaming and processing pipeline using:

- Apache Spark (Structured Streaming)
- Kafka (via Docker)
- S3 (real or simulated via LocalStack)
- Grafana+Prometheus (via Docker) for real-time monitoring
- GitHub Actions for CI
- A modular Makefile for orchestration

It includes:
- A Kafka-based event producer
- A Spark streaming processor and raw events writer
- A percentile computation job
- A Grafana dashboard for monitoring
- Unit tests

---

## âš™ï¸ Pre-requirements

Before running the project, ensure the following are installed on your **Unix-based machine**:

- âœ… Apache Spark **3.5.0 or higher**
- âœ… Docker (with Docker Compose)
- âœ… AWS CLI
- âœ… yq package (apt install yq) for handling yml file in bash

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ config
â”‚Â Â  â”œâ”€â”€ event_schema.yml
â”‚Â Â  â””â”€â”€ producer_config.yml
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ event_producer.py
â”‚Â Â  â”œâ”€â”€ streaming_processor.py
â”‚Â Â  â”œâ”€â”€ streaming_raw_writer.py
â”‚Â Â  â”œâ”€â”€ percentile_processor.py
â”‚Â Â  â”œâ”€â”€ config.yaml
â”‚Â Â  â”œâ”€â”€ jmx_prometheus_javaagent-1.4.0.jar
â”‚Â Â  â””â”€â”€ metrics.properties
â”œâ”€â”€ kafka
â”‚Â Â  â””â”€â”€ docker-compose.yml
â”œâ”€â”€ s3
â”‚Â Â  â”œâ”€â”€ create_bucket.sh
â”‚Â Â  â””â”€â”€ docker-compose.yml
â”œâ”€â”€ grafana
â”‚Â Â  â”œâ”€â”€ dashboards
â”‚Â Â  â”œâ”€â”€ docker-compose.yml
â”‚Â Â  â”œâ”€â”€ prometheus
â”‚Â Â  â””â”€â”€ provisioning
â”œâ”€â”€ tests
    â”œâ”€â”€ integration
    â””â”€â”€ unit
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```
### `config/`
Contains configuration files used by the producer and schema validation:
- **`event_schema.yml`** â†’ Defines the expected structure of incoming IoT events.  
- **`producer_config.yml`** â†’ Contains parameters for the event producer (e.g., Kafka topic, frequency, etc.).  

---

### `src/`
Core source code for streaming and batch processing:
- **`event_producer.py`** â†’ Simulates IoT devices and sends events to Kafka.  
- **`streaming_processor.py`** â†’ Processes events in real-time, applies transformations and aggregations.  
- **`streaming_raw_writer.py`** â†’ Writes raw events to S3 in Parquet format.  
- **`percentile_processor.py`** â†’ Batch job to compute percentiles over historical data.  
- **`config.yaml`** â†’ Prometheus exporter configuration for Spark metrics.  
- **`jmx_prometheus_javaagent-1.4.0.jar`** â†’ Java agent used to expose Spark metrics to Prometheus.  
- **`metrics.properties`** â†’ Spark metrics configuration file.  

---

### `kafka/`
- **`docker-compose.yml`** â†’ Docker Compose setup for Kafka and Zookeeper.  

---

### `s3/`
Docker Compose setup for LocalStack (S3 simulation) and bucket creation:
- **`create_bucket.sh`** â†’ Script to create the S3 bucket used by Spark.  
- **`docker-compose.yml`** â†’ Defines the LocalStack container.  

---

### `grafana/`
Monitoring stack configuration:
- **`dashboards/`** â†’ Predefined Grafana dashboards.  
- **`prometheus/`** â†’ Prometheus configuration files.  
- **`provisioning/`** â†’ Grafana provisioning setup (data sources, dashboards).  
- **`docker-compose.yml`** â†’ Starts Grafana and Prometheus containers.  

---

### `tests/`
Contains unit and integration tests:
- **`unit/`** â†’ Unit tests for individual components.  
- **`integration/`** â†’ Integration tests for end-to-end validation.  

---

### Root files
- **`Makefile`** â†’ Orchestrates the entire pipeline: starting services, running Spark jobs, testing, and cleanup.  
- **`requirements.txt`** â†’ Python dependencies for the producer and test scripts.  
- **`README.md`** â†’ Project documentation.  
- **`LICENSE`** â†’ License file for the project.
