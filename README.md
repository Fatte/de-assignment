# DE Takehome Test: Event Streaming and Processing Pipeline

## ğŸ“– Project Overview

This project implements a complete event streaming and processing pipeline using:

- Apache Spark (Structured Streaming)
- Kafka (via Docker)
- S3 (real or simulated via LocalStack)
- Grafana+Prometheus (via Docker) for real-time monitoring
- GitHub Actions for CI
- A modular Makefile for orchestration

It includes:
- A Kafka-based event producer
- A Spark streaming processor and raw events writer
- A percentile computation job
- A Grafana dashboard for monitoring
- Unit tests

---

## âš™ï¸ Pre-requirements

Before running the project, ensure the following are installed on your **Unix-based machine**:

- âœ… Apache Spark **3.5.0 or higher**
- âœ… Docker (with Docker Compose)
- âœ… AWS CLI

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ config
â”‚Â Â  â”œâ”€â”€ event_schema.yml
â”‚Â Â  â”œâ”€â”€ producer_config.yml
â”‚Â Â  â””â”€â”€ streaming_config.yml
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ event_producer.py
â”‚Â Â  â”œâ”€â”€ streaming_processor.py
â”‚Â Â  â”œâ”€â”€ streaming_raw_writer.py
â”‚Â Â  â”œâ”€â”€ percentile_processor.py
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â””â”€â”€ metrics.properties
â”œâ”€â”€ kafka
â”‚Â Â  â””â”€â”€ docker-compose.yml
â”œâ”€â”€ s3
â”‚Â Â  â”œâ”€â”€ create_bucket.sh
â”‚Â Â  â””â”€â”€ docker-compose.yml
â”œâ”€â”€ grafana
â”‚Â Â  â”œâ”€â”€ dashboards
â”‚Â Â  â”œâ”€â”€ docker-compose.yml
â”‚Â Â  â”œâ”€â”€ prometheus
â”‚Â Â  â””â”€â”€ provisioning
â”œâ”€â”€ tests
    â”œâ”€â”€ integration
    â””â”€â”€ unit
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```
### `config/`
Contains configuration files used by the producer and schema validation:
- **`event_schema.yml`** â†’ Defines the expected structure of incoming IoT events.  
- **`producer_config.yml`** â†’ Contains parameters for the event producer (e.g., Kafka topic, frequency, etc.).  
- **`streaming_config.yml`** â†’ Contains parameters for the streaming processors (e.g., Kafka topic, S3 output paths, etc.). 

---

### `src/`
Core source code for streaming and batch processing:
- **`event_producer.py`** â†’ Simulates IoT devices and sends events to Kafka.  
- **`streaming_processor.py`** â†’ Processes events in real-time, applies transformations and aggregations.  
- **`streaming_raw_writer.py`** â†’ Writes raw events to S3 in Parquet format.  
- **`percentile_processor.py`** â†’ Batch job to compute percentiles over historical data.  
- **`lib`** â†’ Contains Java agent and the confs used to expose Spark metrics to Prometheus.  
- **`metrics.properties`** â†’ Spark metrics configuration file.  

---

### `kafka/`
- **`docker-compose.yml`** â†’ Docker Compose setup for Kafka and Zookeeper.  

---

### `s3/`
Docker Compose setup for LocalStack (S3 simulation) and bucket creation:
- **`create_bucket.sh`** â†’ Script to create the S3 bucket used by Spark.  
- **`docker-compose.yml`** â†’ Defines the LocalStack container.  

---

### `grafana/`
Monitoring stack configuration:
- **`dashboards/`** â†’ Predefined Grafana dashboards.  
- **`prometheus/`** â†’ Prometheus configuration files.  
- **`provisioning/`** â†’ Grafana provisioning setup (data sources, dashboards).  
- **`docker-compose.yml`** â†’ Starts Grafana and Prometheus containers.  

---

### `tests/`
Contains unit and integration tests:
- **`unit/`** â†’ Unit tests for individual components.  
- **`integration/`** â†’ Integration tests for end-to-end validation.  

---

### Root files
- **`Makefile`** â†’ Orchestrates the entire pipeline: starting services, running Spark jobs, testing, and cleanup.  
- **`requirements.txt`** â†’ Python dependencies for the producer and test scripts.  
- **`README.md`** â†’ Project documentation.  
- **`LICENSE`** â†’ License file for the project.

---

## ğŸš€ Runbook

### 1. Clone the repository
Download the project from GitHub and make sure your **Unix-based** machine satisfies the prerequisites described above.

```bash
git clone [<REPOSITORY_URL>](https://github.com/Fatte/de-assignment.git)
cd de-assignment
```

---

### 2. Generate the `.env` file
Create the `.env` file in the `s3` folder with:

```bash
make generate_aws_env
```

- The file contains environment variables (credentials, S3 connection settings, and bucket configuration).
- The generated file is just a **template with placeholder values** â†’ you must update it with your actual credentials and configuration.

---

### 3. Configuration
Customize the parameters in the `config/` directory.  
- **Required**: `streaming_config.yml` â†’ configure the S3 output paths.  

---

### 4. Run the end-to-end pipeline
Start the entire stack with:

```bash
make run
```

This command will sequentially:
- Create the Python **virtual environment**  
- Connect to **S3** and create the bucket (if it does not exist)  
- Set up **Kafka** (create topic + start the event producer)  
- Start **Grafana + Prometheus** at `http://localhost:3000` (username: admin - password: admin)
- Start **Spark Streaming jobs** at `http://localhost:4040`  

---

### 5. Run the batch job (95th percentile)
To run the Spark batch job that computes the 95th percentile:

```bash
make percentile_job
```

---

### 6. Run the tests
To execute the unit tests:

```bash
make test
```

---

## ğŸ“Œ Command Cheat Sheet

| Command                | Description                                                                 |
|-------------------------|-----------------------------------------------------------------------------|
| `make generate_aws_env` | Generate `s3/.env` file with S3 credentials and bucket config (template values) |
| `make run`              | Launch full pipeline: S3 + Kafka + Spark streaming + Grafana/Prometheus      |
| `make percentile_job`   | Run Spark batch job to compute 95th percentile                               |
| `make clean`            | Destroy all the project docker containers                                    |
| `make kill producer`    | Kill the event producer script                                               |
| `make kill processor`   | Kill the streaming pyspark jobs                                              |

